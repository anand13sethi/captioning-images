{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import skimage\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import VGG16\n",
    "from keras.models import Model\n",
    "from keras import backend\n",
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.applications import VGG16\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = pd.read_json('./Dataset/'+filename+'.json')\n",
    "    image_id_list = data.columns\n",
    "    caption_list = []\n",
    "    \n",
    "    for i in image_id_list:\n",
    "        caption_list.append(data[i])\n",
    "    \n",
    "    print(\"Data Size - \", len(image_id_list), \"\\nCaptions Per Image - \", len(caption_list[0]))\n",
    "    \n",
    "    return image_id_list, caption_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size -  118287 \n",
      "Captions Per Image -  5\n"
     ]
    }
   ],
   "source": [
    "image_list_train, caption_list_train = load_data('train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size -  5000 \n",
      "Captions Per Image -  5\n"
     ]
    }
   ],
   "source": [
    "image_list_val, caption_list_val = load_data('val_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_model = VGG16(include_top = True, weights = 'imagenet')\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7f2d5e2b5978>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_output = vgg16_model.get_layer('fc2')\n",
    "dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_vgg16 = Model(inputs = vgg16_model.input, outputs = dense_output.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, size = None):\n",
    "    img = Image.open(path)\n",
    "    \n",
    "    if not size is None:\n",
    "        img = image.img_to_array(img)\n",
    "        img = skimage.transform.resize(img, size)\n",
    "    img = np.array(img)\n",
    "    img = img / 255.0\n",
    "    \n",
    "    if (len(img.shape) == 2):\n",
    "        img = np.repeat(img[:, :, np.newaxis], 3, axis=2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(idx, train, image_id_list, caption_list):\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    if train: \n",
    "        dir_p = cwd+'/Dataset/val_2017/'\n",
    "        filename = image_id_list[idx]\n",
    "        captions = caption_list[idx]\n",
    "    else:\n",
    "        dir_p = cwd+'/Dataset/Test_Image/'\n",
    "        filename = image_id[idx]\n",
    "        captions = caption_list[idx]\n",
    "    path = os.path.join(dir_p,+str(filename)+'.jpg')\n",
    "    \n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    \n",
    "    img = load_image(path)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(data_dir, image_id_list, batch_size):\n",
    "    print(data_dir)\n",
    "    \n",
    "    num_of_images = len(image_id_list)\n",
    "    image_size = backend.int_shape(vgg16_model.input)[1:3]\n",
    "    dense_output_size = backend.int_shape(dense_output.output)[1]\n",
    "    \n",
    "    image_shape = (batch_size,) + image_size + (3,)\n",
    "    print(image_shape)\n",
    "    image_batch = np.zeros(shape = image_shape, dtype = np.float16)\n",
    "    \n",
    "    dense_val_shape = (num_of_images, dense_output_size)\n",
    "    dense_values = np.zeros(shape = dense_val_shape, dtype = np.float16)\n",
    "    \n",
    "    start_index = 0\n",
    "    \n",
    "    while start_index < num_of_images:\n",
    "        print(start_index, end = \"\\r\")\n",
    "        end_index = start_index + batch_size\n",
    "        if end_index > num_of_images:\n",
    "            end_index = num_of_images\n",
    "        \n",
    "        current_batch_size = end_index - start_index\n",
    "        i = 0\n",
    "        for image_id in image_id_list[start_index:end_index]:\n",
    "            \n",
    "            id_len = len(str(image_id))\n",
    "            file_name = ['0'] * 13\n",
    "            file_name[12-id_len:] = str(image_id)\n",
    "            file_name = ''.join(file_name)\n",
    "            file_name += '.jpg'\n",
    "            \n",
    "            path = os.path.join(data_dir, file_name)\n",
    "            img = load_image(path, size = image_size)\n",
    "            image_batch[i] = img\n",
    "            i += 1\n",
    "        dense_values_batch = custom_vgg16.predict(image_batch[0:current_batch_size])\n",
    "        \n",
    "        dense_values[start_index:end_index] = dense_values_batch[0:current_batch_size]\n",
    "            \n",
    "        start_index = end_index\n",
    "    \n",
    "    return dense_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train():\n",
    "    pwd = os.getcwd()\n",
    "    train_path = pwd + '/Dataset/train_2017/'\n",
    "    cache_path = pwd + '/Cache/dense_values_train.pkl'\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as file:\n",
    "            obj = pickle.load(file)\n",
    "    else:\n",
    "        obj = process_images(train_path, image_list_train, 32)\n",
    "        with open(cache_path, 'wb') as file:\n",
    "            pickle.dump(obj, file)\n",
    "    \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_val():\n",
    "    pwd = os.getcwd()\n",
    "    val_path = pwd + '/Dataset/val_2017/'\n",
    "    cache_path = pwd + '/Cache/dense_values_val.pkl'\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as file:\n",
    "            obj = pickle.load(file)\n",
    "    else:\n",
    "        obj = process_images(val_path, image_list_val, 32)\n",
    "        with open(cache_path, 'wb') as file:\n",
    "            pickle.dump(obj, file)\n",
    "    \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kshitij/Desktop/Sem2/SMAI/Project/captioning-images/Dataset/val_2017/\n",
      "(32, 224, 224, 3)\n",
      "Shape -  (5000, 4096)\n"
     ]
    }
   ],
   "source": [
    "dense_values_val = process_val()\n",
    "print(\"Shape - \", dense_values_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = os.getcwd()\n",
    "cache_path = pwd + '/Cache/dense_values_val.pkl'\n",
    "transfer_values = None\n",
    "with open(cache_path, 'rb') as file:\n",
    "    transfer_values = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(train=True):\n",
    "\n",
    "    if train:\n",
    "        filename = 'Dataset/captions_train2017.json'\n",
    "    else:\n",
    "        filename = 'Dataset/captions_val2017.json'\n",
    "\n",
    "    path = filename\n",
    "    \n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data_raw = json.load(file)\n",
    "\n",
    "    images = data_raw['images']\n",
    "    annotations = data_raw['annotations']\n",
    "    \n",
    "    records = dict()\n",
    "\n",
    "    for image in images:\n",
    "        image_id = image['id']\n",
    "        filename = image['file_name']\n",
    "        \n",
    "        record = dict()\n",
    "        \n",
    "        record['filename'] = filename\n",
    "        \n",
    "        record['captions'] = list()\n",
    "        \n",
    "        records[image_id] = record\n",
    "        \n",
    "    for ann in annotations:\n",
    "        \n",
    "        image_id = ann['image_id']\n",
    "        caption = ann['caption']\n",
    "        \n",
    "        record = records[image_id]\n",
    "        \n",
    "        record['captions'].append(caption)\n",
    "        \n",
    "    records_list = [(key, record['filename'], record['captions'])\n",
    "                    for key, record in sorted(records.items())]\n",
    "    \n",
    "    ids, filenames, captions = zip(*records_list)\n",
    "\n",
    "    return ids, filenames, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, filenames_train, captions_train = load(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_start = 'ssss '\n",
    "mark_end = ' eeee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_captions(captions_listlist):\n",
    "    captions_marked = [[mark_start + caption + mark_end\n",
    "                        for caption in captions_list]\n",
    "                        for captions_list in captions_listlist]\n",
    "    \n",
    "    return captions_marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ssss A woman stands in the dining area at the table. eeee',\n",
       " 'ssss A room with chairs, a table, and a woman in it. eeee',\n",
       " 'ssss A woman standing in a kitchen by a window eeee',\n",
       " 'ssss A person standing at a table in a room. eeee',\n",
       " 'ssss A living area with a television and a table eeee']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_train_marked = mark_captions(captions_train)\n",
    "captions_train_marked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A woman stands in the dining area at the table.',\n",
       " 'A room with chairs, a table, and a woman in it.',\n",
       " 'A woman standing in a kitchen by a window',\n",
       " 'A person standing at a table in a room.',\n",
       " 'A living area with a television and a table']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(captions_listlist):\n",
    "    captions_list = [caption\n",
    "                     for captions_list in captions_listlist\n",
    "                     for caption in captions_list]\n",
    "    \n",
    "    return captions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_train_flat = flatten(captions_train_marked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrap(Tokenizer):\n",
    "    \n",
    "    def __init__(self, texts, num_words=None):\n",
    "\n",
    "        Tokenizer.__init__(self, num_words=num_words)\n",
    "        \n",
    "        self.fit_on_texts(texts)\n",
    "        \n",
    "        self.index_to_word = dict(zip(self.word_index.values(), self.word_index.keys()))\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word \n",
    "\n",
    "    def tokens_to_string(self, tokens):\n",
    "        \n",
    "        words = [self.index_to_word[token]\n",
    "                 for token in tokens\n",
    "                 if token != 0]\n",
    "        \n",
    "        text = \" \".join(words)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def captions_to_tokens(self, captions_listlist):\n",
    "        \n",
    "        tokens = [self.texts_to_sequences(captions_list)\n",
    "                  for captions_list in captions_listlist]\n",
    "        \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TokenizerWrap(texts=captions_train_flat, num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_start = tokenizer.word_index[mark_start.strip()]\n",
    "token_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_end = tokenizer.word_index[mark_end.strip()]\n",
    "token_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = tokenizer.captions_to_tokens(captions_train_marked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1, 22, 171, 7, 6, 444, 106, 18, 6, 24, 3],\n",
       " [2, 1, 45, 8, 295, 1, 24, 9, 1, 22, 7, 27, 3],\n",
       " [2, 1, 22, 16, 7, 1, 68, 50, 1, 132, 3],\n",
       " [2, 1, 30, 16, 18, 1, 24, 7, 1, 45, 3],\n",
       " [2, 1, 119, 106, 8, 1, 288, 9, 1, 24, 3]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ssss A woman stands in the dining area at the table. eeee',\n",
       " 'ssss A room with chairs, a table, and a woman in it. eeee',\n",
       " 'ssss A woman standing in a kitchen by a window eeee',\n",
       " 'ssss A person standing at a table in a room. eeee',\n",
       " 'ssss A living area with a television and a table eeee']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_train_marked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
